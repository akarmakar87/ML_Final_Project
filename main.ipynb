{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Members: Asha Karmakar, Akhil Iyer, Megan Sundheim, Grace Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, cophenet\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Preprocessing & Exploration </h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>TODO: explain what we are doing, why, and whether it worked</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in csv file\n",
    "df = pd.read_csv(\"./data/genres.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "# Remove the following columns: type, id, uri, track_href, analysis_url, unnamed, title, song_name\n",
    "df = df[['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature', 'genre']]\n",
    "df_dt = df.copy() # Saved for decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One hot encode time signature\n",
    "df = pd.get_dummies(df, columns=['time_signature'], drop_first=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate data into features and class labels\n",
    "features = df.drop('genre', axis=1)\n",
    "labels = df['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(features.head())\n",
    "print()\n",
    "label_groups = df.groupby('genre')\n",
    "unique_genres = sorted(list(set(list(labels))))\n",
    "print(label_groups.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are no missing values:\n",
    "null_data = df[df.isnull().any(axis=1)]\n",
    "len(null_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot correlation between features.\n",
    "ax = sns.heatmap(\n",
    "    features.corr(), \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform anomaly detection.\n",
    "y_pred_list = []\n",
    "\n",
    "# Probabilistic anomaly detection.\n",
    "envelope_pred = EllipticEnvelope().fit_predict(features)\n",
    "y_pred_list.append(envelope_pred)\n",
    "\n",
    "# Proximity-based anomaly detection.\n",
    "forest_pred = IsolationForest().fit_predict(features)\n",
    "y_pred_list.append(forest_pred)\n",
    "\n",
    "# Density-based anomaly detection.\n",
    "lof_pred = LocalOutlierFactor().fit_predict(features)\n",
    "y_pred_list.append(lof_pred)\n",
    "\n",
    "y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop anomalous points.\n",
    "features_no_anomalies = features.copy()\n",
    "labels_no_anomalies = labels.copy()\n",
    "anomaly_indices = set()\n",
    "for arr in y_pred_list:\n",
    "    c = Counter(arr)\n",
    "    print(c)\n",
    "    i = len(arr) - 1\n",
    "    while i >= 0:\n",
    "        if arr[i] == -1:\n",
    "            anomaly_indices.add(i)\n",
    "        i -= 1\n",
    "        \n",
    "# Drop anomalies indicated by any of the 3 anomaly detection algorithms.\n",
    "anomaly_indices = list(anomaly_indices)\n",
    "anomaly_indices.sort()\n",
    "features_no_anomalies = features_no_anomalies.drop(anomaly_indices, axis=0)\n",
    "labels_no_anomalies = labels_no_anomalies.drop(anomaly_indices, axis=0)\n",
    "\n",
    "features_no_anomalies = features_no_anomalies.reset_index().drop('index', axis=1)\n",
    "labels_no_anomalies = labels_no_anomalies.reset_index().drop('index', axis=1)\n",
    "label_groups = labels_no_anomalies.groupby('genre')\n",
    "print()\n",
    "print(label_groups.size())\n",
    "print(\"\\nSize of new dataset: %i\" % len(features_no_anomalies))\n",
    "\n",
    "# Display class imbalance with bar graph.\n",
    "sorted_labels = label_groups.size().sort_values(ascending=False)\n",
    "sorted_labels = sorted_labels.reset_index()\n",
    "plt.bar(sorted_labels['genre'], sorted_labels[0])\n",
    "plt.title('Class Imbalance in Non-Anomalous Song Data')\n",
    "plt.xlabel('Music Genres')\n",
    "plt.ylabel('Number of Non-Anomalous Data Points')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified code from scatter() function from HW 5.\n",
    "# Print the labeled data clusters after anomalies taken out.\n",
    "\n",
    "# Scale non-anomalous data.\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_no_anomalies)\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "features_scaled = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Sample subset of scaled data points.\n",
    "numPoints = 1000\n",
    "numEntries = features_scaled.shape[0]\n",
    "samp_records = []\n",
    "samp_labels = []\n",
    "for i in range(numPoints):\n",
    "    index = random.randint(0, numEntries - 1)\n",
    "    samp_records.append(features_scaled[index, :])\n",
    "    samp_labels.append(labels_no_anomalies.iloc[index, 0])\n",
    "\n",
    "# Generate different color for each genre.\n",
    "label_colors = []\n",
    "color_dict = dict()\n",
    "for label in samp_labels:\n",
    "    if not label in color_dict.keys():\n",
    "        # Originally found in https://stackoverflow.com/questions/28999287/generate-random-colors-rgb.\n",
    "        color = \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "        color_dict[label] = color\n",
    "    label_colors.append(color_dict[label])\n",
    "\n",
    "# Project sampled data down onto 2 dimensions to visualize similarity.\n",
    "mds = MDS(n_components=2)\n",
    "mds_data = mds.fit_transform(samp_records)\n",
    "\n",
    "# Assign label for each data point.\n",
    "classes = dict()\n",
    "for i in range(len(mds_data)):\n",
    "    record = mds_data[i]\n",
    "    label = samp_labels[i]\n",
    "    if not label in classes.keys():\n",
    "        classes[label] = [record]\n",
    "    else:\n",
    "        classes[label].append(record)\n",
    "\n",
    "# Plot cluster of sampled data for each individual genre.\n",
    "fig, axes = plt.subplots(len(classes.keys()) + 1, 1, figsize=(6, 100), sharey=True, sharex=True)\n",
    "index = 1\n",
    "for genre in classes.keys():\n",
    "    ax = axes[index]\n",
    "    index += 1\n",
    "    data = pd.DataFrame(classes[genre])\n",
    "    color = [color_dict[genre] for i in range(len(data))]\n",
    "    axes[0].scatter(data.iloc[:, 0], data.iloc[:, 1], c=color, s=50, label=genre)\n",
    "    ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=color, s=50, label=genre)\n",
    "\n",
    "# Originally found in https://towardsdatascience.com/legend-outside-the-plot-matplotlib-5d9c1caa9d31#:~:text=Placing%20the%20legend%20outside%20of%20the%20plot&text=First%2C%20we%20need%20to%20shrink,control%20for%20manual%20legend%20placement.&text=Box%20that%20is%20used%20to,Defaults%20to%20axes.\n",
    "axes[0].legend(loc='center right', bbox_to_anchor=(1.4, 0.5))\n",
    "index = 1\n",
    "\n",
    "# Plot all genres onto one scatterplot.\n",
    "while index <= len(classes.keys()):\n",
    "    ax = axes[index]\n",
    "    ax.legend(loc='best')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Building </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "As part of our group's brainstorming process for which classifier types would be most appropriate to train our data on, we chose to try Naive Bayes since it is not susceptible to the curse of dimensionality (and we have a quite a few features we are using). Since Naive Bayes assumes that all features are conditionally independent of each other given the label and our features were weakly correlated with each other, we thought this was an additional reason this classifier could be appropriate.\n",
    "\n",
    "Gaussian Naive Bayes was the only subtype of this classifier that we were able to use since it allowed for use of continuous feature values. Multinomial and Categorical required discrete feature values, Complement needed nonnegative values, and Bernoilli relied on binary feature values, so we couldn't use any of these while using all of our features (original and reduced dimensionality).\n",
    "\n",
    "We first trained 2 different Gaussian Naive Bayes classifiers with 10-fold cross validation for all of our data and \n",
    "for our data without the anomalies detected during data exploration and cleaning. This was to determine how much anomalies skewed the classifier's feature probability distributions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Gaussian Naive Bayes classifier.\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Run 10-fold cross validation on classifier.\n",
    "accuracies = cross_val_score(gnb, features, list(labels), cv=10)\n",
    "\n",
    "# Print the accuracy of the cross-validated model.\n",
    "avg_accuracy = accuracies.mean()\n",
    "print(\"Accuracy:\", (avg_accuracy * 100))\n",
    "\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier for data without anomalies.\n",
    "gnb_no_anomalies = GaussianNB()\n",
    "labels_no_anomalies = labels_no_anomalies.iloc[:, 0]\n",
    "\n",
    "# Run 10-fold cross validation on classifier for data without anomalies.\n",
    "accuracies_no_anomalies = cross_val_score(gnb_no_anomalies, features_no_anomalies, labels_no_anomalies, cv=10)\n",
    "\n",
    "# Print the accuracy of the cross-validated model for data without anomalies.\n",
    "avg_accuracy_no_anomalies = accuracies_no_anomalies.mean()\n",
    "print(\"Accuracy without anomalies:\", (avg_accuracy_no_anomalies * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our 15 genres to choose from in classification of a song in the dataset, a random classifier would have about a 6.67% chance of choosing a certain genre. Therefore, with our whole data classifier yielding ~49.3% accuracy and the non-anomalous classifier yielding ~52.3% accuracy, both classifiers performed significantly better than random. \n",
    "\n",
    "Also worth noting is the improvement in the accuracy of the classifier by 3% after removing anomalies, indicating that the of the ~8000 points we removed, there was some skewing of classes by anomalous feature values.\n",
    "\n",
    "Due to the severe class imbalance between genres in this dataset, accuracy may not be an accurate depiction of the spread of classification performance across classes. Therefore, we wanted to take a look at the classification matrix and report of other evaluation metrics to investigate the performance of our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix in a more visually appealing heat map.\n",
    "def plot_confusion_matrix(confusion_mat):\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(confusion_mat, annot=True, fmt='g', ax=ax)\n",
    "    \n",
    "    unique_labels = sorted(list(set(list(labels))))\n",
    "\n",
    "    # Labels, title and ticks.\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(unique_genres, rotation=90)\n",
    "    ax.yaxis.set_ticklabels(unique_genres, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from Megan Sundheim's HW 4.\n",
    "\n",
    "# Print the confusion matrix for the whole data classifier.\n",
    "pred_labels = cross_val_predict(gnb, features, labels, cv=10)\n",
    "con_mat = confusion_matrix(labels, pred_labels)\n",
    "plot_confusion_matrix(con_mat)\n",
    "\n",
    "# Display the classification report for the whole data Naive Bayes classifier.\n",
    "report = classification_report(labels, pred_labels, zero_division=0)\n",
    "print(\"\\nClassification Report with Anomalies:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from Megan Sundheim's HW 4.\n",
    "\n",
    "# Print the confusion matrix for the classifier without anomalies.\n",
    "pred_labels_no_anomalies = cross_val_predict(gnb_no_anomalies, features_no_anomalies, labels_no_anomalies, cv=10)\n",
    "con_mat_no_anomalies = confusion_matrix(labels_no_anomalies, pred_labels_no_anomalies)\n",
    "plot_confusion_matrix(con_mat_no_anomalies)\n",
    "\n",
    "# Display the classification report for the Naive Bayes classifier without anomalies.\n",
    "report_no_anomalies = classification_report(labels_no_anomalies, pred_labels_no_anomalies, zero_division=0)\n",
    "report_no_anomalies_dict = classification_report(labels_no_anomalies, pred_labels_no_anomalies, zero_division=0, output_dict=True)\n",
    "print(\"\\nClassification Report without Anomalies:\\n\", report_no_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the 2 classification reports, both classifiers seem to especially struggle with Emo, Pop, and RnB, having classified 0 songs as these genres, despite more than a 1000 songs in either dataset being Emo and RnB and all of these genres being relatively distinct and recognizable to the human ear. Similarly surprising was the f1 score being unusually low (0.06 with anomalies and 0.02 without) for Dark Trap despite having the 2nd largest amount of songs in this genre in the data. Psytrance and dnb had f1 scores from 0.83-0.85 in either classifier despite a moderate amount of points in each genre.\n",
    "\n",
    "From these observations, we decided to investigate the relationship between cohesion (with relation to number of points in each genre) of the non-anomalous classes and the f1 scores with Gaussian Naive Bayes. Since Naive Bayes assumes that each feature is independent of each other and relies on probability distributions for each feature, perhaps the so-so performance could be attributed to similar feature values across genres.\n",
    "\n",
    "Also, due to the variety in f1 scores across imbalanced genres, we decided to try Naive Bayes with a downsampled dataset with non-anomalous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_cohesion = dict()\n",
    "df_no_anomalies = pd.DataFrame(features_no_anomalies.copy())\n",
    "df_no_anomalies['genre'] = labels_no_anomalies\n",
    "\n",
    "# Calculate the cohesion (similarity) between points in each genre.\n",
    "for genre in classes.keys():\n",
    "    # Sum WSS over each genre.\n",
    "    cohesion = 0\n",
    "    data = df_no_anomalies[df_no_anomalies['genre'] == genre].drop('genre', axis=1)\n",
    "    centroid = np.mean(data, axis=0)\n",
    "    for i in range(0, len(data)):\n",
    "        # Sum WSS over each data point in current cluster.\n",
    "        record = data.iloc[i, :]\n",
    "        cohesion += (math.sqrt(((record[0] - centroid[0]) ** 2) + (record[1] - centroid[1]) ** 2)) ** 2\n",
    "    print(\"The cohesion of records in %s is %f.\" % (genre, cohesion))\n",
    "    print(\"The cohesion of records in %s, relative to size, is %f.\\n\" % (genre, (cohesion / len(data))))\n",
    "    rel_cohesion[genre] = cohesion / len(data)\n",
    "    \n",
    "# Sum WSS over all genres.\n",
    "total_cohesion = 0\n",
    "total_c = np.mean(features_no_anomalies, axis=0)\n",
    "for i in range(len(features_no_anomalies)):\n",
    "    record = features_no_anomalies.iloc[i, :]\n",
    "    total_cohesion += (math.sqrt(((record[0] - total_c[0]) ** 2) + (record[1] - total_c[1]) ** 2)) ** 2\n",
    "\n",
    "rel_total = total_cohesion / len(features_no_anomalies)\n",
    "print(\"The cohesion of all records is %f.\" % (total_cohesion))\n",
    "print(\"The relative cohesion of all records is %f.\" % (rel_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation between Naive Bayes f1 score and relative cohesion of class for non-anomalous data.\n",
    "genre_f1 = dict()\n",
    "genres = df.groupby('genre').groups.keys()\n",
    "corr_df = []\n",
    "for genre in genres:\n",
    "    metrics = report_no_anomalies_dict[genre]\n",
    "    f1 = metrics['f1-score']\n",
    "    corr_df.append([f1, rel_cohesion[genre]])\n",
    "\n",
    "corr_df = pd.DataFrame(corr_df, index=genres, columns=['f1-score', 'Relative Cohesion'])\n",
    "print(corr_df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1 score from Naive Bayes classification and relative cohesion (without anomalies) of each genre appears to be moderately to highly negatively correlated. Cohesion measures the proximity of each point in a cluster to its centroid, so lower cohesion of songs within their actual genres means that songs are more similar to each other. F1 score combines the precision and recall metrics of classification so that a score of 0 is the worst performance and 1 is the best. Therefore, it makes sense for our performance metric f1 score to be negatively correlated to cohesion, as our classifier should have an easier time discerning between genres (higher f1 score) when songs in a genre tend to be more similar to each other (lower cohesion).\n",
    "\n",
    "Therefore, the performance of our Naive Bayes classifier seems to be moderately to highly influenced by the similarity of songs within a genre.\n",
    "\n",
    "We will lastly train a Gaussian Naive Bayes classifier to determine the effect of class imbalance on the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Naive Bayes on sampled data.\n",
    "gnb_sampled = GaussianNB()\n",
    "\n",
    "# Run 10-fold cross validation on classifier.\n",
    "accuracies = cross_val_score(gnb_sampled, samp_features, samp_genres.iloc[:, 0], cv=10)\n",
    "\n",
    "# Print the accuracy of the cross-validated model.\n",
    "avg_accuracy = accuracies.mean()\n",
    "print(\"Accuracy with Sampled/Balanced Non-Anomalous Data:\", (avg_accuracy * 100))\n",
    "\n",
    "\n",
    "# Original code from Megan Sundheim's HW 4.\n",
    "\n",
    "# Print the confusion matrix for the classifier with balanced classes.\n",
    "pred_labels_sampled = cross_val_predict(gnb_sampled, samp_features, samp_genres.iloc[:, 0], cv=10)\n",
    "con_mat_sampled = confusion_matrix(samp_genres, pred_labels_sampled)\n",
    "plot_confusion_matrix(con_mat_sampled)\n",
    "\n",
    "# Display the classification report for the Naive Bayes classifier with balanced classes.\n",
    "report_sampled = classification_report(samp_genres, pred_labels_sampled, zero_division=0)\n",
    "print(\"\\nClassification Report with Sampled/Balanced Non-Anomalous Data:\\n\", report_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the completely balanced downsampled non-anomalous data, the accuracy of our classifer decreased by ~3% and ~6% from our whole data and non-anomalous classifiers, respectively. Although the accuracy decreased, this may actually indicate a more honest and fair representation of the classifier's performance on the dataset.\n",
    "\n",
    "\n",
    "Notably, all genres receive some number of correct predictions (as opposed to Emo, Pop, and Rnb receiving no predictions in the previous two classifiers). Psytrance and dnb still remain as the best performing classes (f1 scores of 0.82 and 0.80, respectively), however Underground Rap saw a huge hit in its performance (a drop of ~0.30). This genre had the most songs in it originally, perhaps indicating a skew in genre performance based on number of points available in the dataset. \n",
    "\n",
    "Overall, the best performing Naive Bayes classifier was most likely the non-anomalous, imbalanced classifier (with the highest weighted precision, recall, and f1 score overall). However, after balancing the data via downsampling, the imbalanced classifier may have been skewed to improve accuracy in majority classes and ignore minority classes in its training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets for Decision Trees (No One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_dt = df_dt.drop('genre', axis=1)\n",
    "labels_dt = df_dt['genre']\n",
    "\n",
    "y_pred_list_dt = []\n",
    "\n",
    "envelope_pred_dt = EllipticEnvelope().fit_predict(features_dt)\n",
    "y_pred_list_dt.append(envelope_pred_dt)\n",
    "\n",
    "# apply IsolationForest AD\n",
    "forest_pred_dt = IsolationForest().fit_predict(features_dt)\n",
    "y_pred_list_dt.append(forest_pred_dt)\n",
    "\n",
    "# apply LocalOutlierFactor AD\n",
    "lof_pred_dt = LocalOutlierFactor().fit_predict(features_dt)\n",
    "y_pred_list_dt.append(lof_pred_dt)\n",
    "\n",
    "features_no_anomalies_dt = features_dt.copy()\n",
    "labels_no_anomalies_dt = labels_dt.copy()\n",
    "anomaly_indices_dt = set()\n",
    "for arr in y_pred_list_dt:\n",
    "    c = Counter(arr)\n",
    "    print(c)\n",
    "    i = len(arr) - 1\n",
    "    while i >= 0:\n",
    "        if arr[i] == -1:\n",
    "            anomaly_indices_dt.add(i)\n",
    "        i -= 1\n",
    "        \n",
    "# Drop anomalies indicated by any of the 3 anomaly detection algorithms.\n",
    "anomaly_indices_dt = list(anomaly_indices_dt)\n",
    "anomaly_indices_dt.sort()\n",
    "features_no_anomalies_dt = features_no_anomalies_dt.drop(anomaly_indices_dt, axis=0)\n",
    "labels_no_anomalies_dt = labels_no_anomalies_dt.drop(anomaly_indices_dt, axis=0)\n",
    "\n",
    "features_no_anomalies_dt = features_no_anomalies_dt.reset_index().drop('index', axis=1)\n",
    "labels_no_anomalies_dt = labels_no_anomalies_dt.reset_index().drop('index', axis=1)\n",
    "label_groups_dt = labels_no_anomalies_dt.groupby('genre')\n",
    "\n",
    "min_size_dt = math.inf\n",
    "for genre in label_groups_dt.groups.keys():\n",
    "    genre_group_dt = label_groups_dt.get_group(genre)\n",
    "    if len(genre_group_dt) < min_size_dt:\n",
    "        min_size_dt = len(genre_group_dt)\n",
    "num_samples_dt = min_size_dt\n",
    "\n",
    "# Sample same number of points from each genre to balance classes.\n",
    "samp_features_dt = []\n",
    "samp_genres_dt = []\n",
    "samp_records_dt = []\n",
    "df_no_anomalies_dt = pd.DataFrame(features_no_anomalies_dt.copy())\n",
    "df_no_anomalies_dt['genre'] = labels_no_anomalies_dt\n",
    "genres_dt = df_no_anomalies_dt.groupby('genre')\n",
    "for genre in genres_dt.groups.keys():\n",
    "    genre_group_dt = genres.get_group(genre)\n",
    "    genre_group_copy_dt = genre_group_dt.copy()\n",
    "    genre_group_copy_dt = genre_group_copy_dt.reset_index()\n",
    "    for i in range(num_samples_dt):\n",
    "        index = random.randint(0, len(genre_group_copy_dt) - 1)\n",
    "        samp_records_dt.append(genre_group_copy_dt.iloc[index, :])\n",
    "\n",
    "samp_features_dt = pd.DataFrame(samp_records_dt).set_index('index')\n",
    "samp_genres_dt = samp_features_dt['genre']\n",
    "samp_features_dt = samp_features_dt.drop('genre', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decision_tree_clf(features, labels):\n",
    "    dt_model = tree.DecisionTreeClassifier()\n",
    "    parameters = {'criterion':[\"gini\", \"entropy\"], 'max_depth':[5, 10, 15, 20], 'min_samples_leaf':[5, 10, 15, 20], 'max_features': [5, 10, 15]}\n",
    "    dt_grid = GridSearchCV(dt_model, parameters, scoring=\"accuracy\", cv=10)\n",
    "    \n",
    "    y_pred = cross_val_predict(dt_grid, features, labels, cv=10)\n",
    "    \n",
    "    print(classification_report(labels, y_pred))\n",
    "    \n",
    "    confusion_mat = confusion_matrix(labels, y_pred)\n",
    "    plot_confusion_matrix(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Looking at data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize label distribution in bar graph\n",
    "labels_size = list(label_groups_dt.size())\n",
    "\n",
    "genre_size = {k: v for k, v in zip(unique_genres, labels_size)}\n",
    "sorted_f1_genre = {k: v for k, v in sorted(genre_size.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "x = list(sorted_f1_genre.keys())\n",
    "y = list(sorted_f1_genre.values())\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Plot F1 score by genere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_f1_score(class_report):\n",
    "    f1_genre = {genre: class_report[genre]['f1-score'] for genre in unique_genres}\n",
    "    sorted_f1_genre = {k: v for k, v in sorted(f1_genre.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    x = list(sorted_f1_genre.keys())\n",
    "    y = list(sorted_f1_genre.values())\n",
    "\n",
    "    plt.bar(x, y)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('F1-score')\n",
    "    plt.title('F1-score for each genre by decision tree classifier')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return f1_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cohesion_corr(df, f1_genre):\n",
    "    rel_cohesion = dict()\n",
    "    for genre in unique_genres:\n",
    "        cohesion = 0\n",
    "        data = df[df['genre'] == genre].drop('genre', axis=1)\n",
    "        centroid = np.mean(data, axis=0)\n",
    "        for i in range(0, len(data)):\n",
    "            record = data.iloc[i, :]\n",
    "            cohesion += (math.sqrt(((record[0] - centroid[0]) ** 2) + (record[1] - centroid[1]) ** 2)) ** 2\n",
    "        rel_cohesion[genre] = cohesion / len(data)\n",
    "\n",
    "    sorted_cohesion = {k: v for k, v in sorted(rel_cohesion.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    x = list(sorted_cohesion.keys())\n",
    "    y = list(sorted_cohesion.values())\n",
    "\n",
    "    plt.bar(x, y)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Cohesion')\n",
    "    plt.title('Cohesion for each genre')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    f1_genre_values = list(f1_genre.values())\n",
    "    cohesion_values = list(rel_cohesion.values())\n",
    "\n",
    "    plt.scatter(cohesion_values, f1_genre_values)\n",
    "\n",
    "    plt.title(\"Scatterplot of Cohesion and F1 score\")\n",
    "    plt.xlabel(\"Cohesion\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Correlation Coefficient between Cohesion and F1 score\", np.corrcoef(f1_genre_values, cohesion_values)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def separation_corr(df, f1_genre):\n",
    "    overall_centroid = np.mean(samp_features_dt, axis=0)\n",
    "    separation_stats = dict()\n",
    "    centroid_dict = dict()\n",
    "    for genre in unique_genres:\n",
    "        data = df[df['genre'] == genre].drop('genre', axis=1)\n",
    "        centroid_dict[genre] = np.mean(data, axis=0)\n",
    "        crnt_centroid = np.mean(data, axis=0)\n",
    "        dist = math.dist(overall_centroid, crnt_centroid) ** 2\n",
    "        data_size = len(data)\n",
    "        bss = dist * data_size\n",
    "        separation_stats[genre] = bss\n",
    "\n",
    "    sorted_separation = {k: v for k, v in sorted(separation_stats.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    x = list(sorted_separation.keys())\n",
    "    y = list(sorted_separation.values())\n",
    "\n",
    "    plt.bar(x, y)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Separation')\n",
    "    plt.title('Separation for each genre')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    f1_genre_values = list(f1_genre.values())\n",
    "    separation_values = list(separation_stats.values())\n",
    "\n",
    "    plt.scatter(separation_values, f1_genre_values)\n",
    "\n",
    "    plt.title(\"Scatterplot of Seperation and F1 score\")\n",
    "    plt.xlabel(\"Seperation\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Correlation Coefficient between Sepearation and F1 score\", np.corrcoef(f1_genre_values, separation_values)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train with data without anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running decision tree in dataset without anomalies\n",
    "dt_report = decision_tree_clf(features_no_anomalies_dt, labels_no_anomalies_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_genre = plot_f1_score(dt_report)\n",
    "cohesion_corr(df, f1_genre)\n",
    "separation_corr(df, f1_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running decision tree after downsampling\n",
    "dt_report = decision_tree_clf(samp_features_dt, samp_genres_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_genre = plot_f1_score(dt_report)\n",
    "downsample_df = pd.concat([samp_features_dt, samp_genres_dt], axis=1)\n",
    "cohesion_corr(downsample_df, f1_genre)\n",
    "separation_corr(downsample_df, f1_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that the decision tree model would be ideal for our task because we have lots of features and the decision tree model doesn't suffer from the curse of dimensionliaty. Also, we have combination of numerical and categorical features which makes our dataset ideal for using decision tree model.\n",
    "\n",
    "We first tried to train the model on full dataset by splitting the dataset to train (.8) and test set (.2). We used grid search to find the most optimal hyperparameters for criterion, max_depth, min_samples_leaf, and max_features and use that hyperparameters to train the new model with train dataset and test it on the test dataset. We noticed that the certain genres achieve high accuracy while the others don't. Especially, the genres with more data tends to get higher accuracy. For example, the f-1 score of genre Pop was the lowest, and the number of data in original dataset was also the lowest.\n",
    "\n",
    "Therefore, we downsampled the data to account for the class imbalance. After downsampling, we see that F-1 scores of genres are more evenly spread out. There are still genres that the model doesn't perform well. However, this is likely to due to the inherent characteristic of genre. For example, certain genre might be harder to distinguish than other genres because the values of features are similar to that of data from other genres.\n",
    "\n",
    "Although the overall accuracy went down, we observe that genres that didnâ€™t perform well before downsampling such as Pop perform much better.\n",
    "\n",
    "We test whether there is correlation between WSS (Cohesion) and BSS (Separation) between how well the model performs on each genere (F1-score). For downsampled data, we get -0.82 as correlation coefficient between Cohesion and F1-score and 0.29 as that between Separation and F1-score. We see that there is stronger negative correlation between how cohesitive the data points in genre are and how well the model performs on that genre. We see there is positive correlation between how separable genre is from other genres and how well the model performs on that genre. However, there doesn't seem to be great positive correlation as the number is small and there are lots of genres where the WSS is close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_forest_clf(features, labels):\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_param = {'n_estimators': list(range(50, 150, 50))}\n",
    "    \n",
    "    rf_grid = GridSearchCV(rf_model, rf_param, scoring=\"accuracy\", cv=10)\n",
    "    y_pred = cross_val_predict(rf_grid, features, labels, cv=10)\n",
    "    \n",
    "    print(classification_report(labels, y_pred))\n",
    "    \n",
    "    confusion_mat = confusion_matrix(labels, y_pred)\n",
    "    plot_confusion_matrix(confusion_mat)\n",
    "    \n",
    "    # Importance of features\n",
    "    importances = rf_grid.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
    "    forest_importances = pd.Series(importances, index=list(features.columns))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_report = random_forest_clf(samp_features_dt, samp_genres_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_f1_genre = plot_f1_score(rf_report)\n",
    "downsample_df = pd.concat([samp_features_dt, samp_genres_dt], axis=1)\n",
    "cohesion_corr(downsample_df, rf_f1_genre)\n",
    "separation_corr(downsample_df, rf_f1_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use random forest model because it is suitable for our dataset with same reason as decision tree. Random forest model is supposed to be better than decision tree because it predicts label by ensembling multiple decision trees and randomly selects a subset of input features rather than using all features for split. Thefore is better at not overfitting by relying on single best feature.\n",
    "\n",
    "We see that random forest model performs much better than a single decision tree as the model's accuracy on test dataset went up signitificantly. When using only single decision tree, we got accuracy of 0.58. On same dataset, we get accuracy of 0.68.\n",
    "\n",
    "We identify what features were most important by using mean decrease in impurity metrics. We see that tempo is the most imporatnt feature followed by duration of song as second most important feature. Features like mode and time signature were the least important features.\n",
    "\n",
    "We see similar trend between cohesion measure and the model's performance on each genre and so as the separation measure. However, the absolute values of the correlation coefficient both went down."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mlp_orig = MLPClassifier(random_state=1, early_stopping=True, max_iter=50)\n",
    "\n",
    "mlp_param = {\n",
    "    'mlp__hidden_layer_sizes': list(range(90, 110, 10)),\n",
    "    'mlp__activation': ('logistic', 'tanh', 'relu'),\n",
    "    'mlp__solver': ('sgd','adam'),\n",
    "    'mlp__alpha': (0.0001, 0.05),\n",
    "    'mlp__learning_rate': ('constant','adaptive') \n",
    "}\n",
    "\n",
    "mlp_orig_pipe = Pipeline([('scaler', scaler), ('mlp', mlp_orig)])\n",
    "mlp_orig_grid = GridSearchCV(mlp_orig_pipe, mlp_param, n_jobs=-1, cv=5)\n",
    "y_orig_pred = cross_val_predict(mlp_orig_grid, features, labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results on the entire dataset:')\n",
    "print(classification_report(labels, y_orig_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix for the original dataset:')\n",
    "cm_orig = metrics.confusion_matrix(labels,y_orig_pred)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm_orig, annot=True, fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_noanom = MLPClassifier(random_state=1, early_stopping=True, max_iter=100)\n",
    "\n",
    "mlp_param = {\n",
    "    'mlp__hidden_layer_sizes': list(range(90, 110, 10)),\n",
    "    'mlp__activation': ('logistic', 'tanh', 'relu'),\n",
    "    'mlp__solver': ('sgd','adam'),\n",
    "    'mlp__alpha': (0.0001, 0.05),\n",
    "    'mlp__learning_rate': ('constant','adaptive') \n",
    "}\n",
    "\n",
    "mlp_noanom_pipe = Pipeline([('scaler', scaler), ('mlp', mlp_noanom)])\n",
    "mlp_noanom_grid = GridSearchCV(mlp_noanom_pipe, mlp_param, n_jobs=-1, cv=5)\n",
    "y_noanom_pred = cross_val_predict(mlp_noanom_grid, features_no_anomalies, labels_no_anomalies, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results on the dataset without anomalies:')\n",
    "print(classification_report(labels_no_anomalies, y_noanom_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_noanom = metrics.confusion_matrix(labels_no_anomalies,y_noanom_pred)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm_noanom, annot=True, fmt='g')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mlp_down = MLPClassifier(random_state=1, early_stopping=True, max_iter=50)\n",
    "\n",
    "mlp_param = {\n",
    "    'mlp__hidden_layer_sizes': list(range(90, 110, 10)),\n",
    "    'mlp__activation': ('logistic', 'tanh', 'relu'),\n",
    "    'mlp__solver': ('sgd','adam'),\n",
    "    'mlp__alpha': (0.0001, 0.05),\n",
    "    'mlp__learning_rate': ('constant','adaptive') \n",
    "}\n",
    "\n",
    "mlp_down_pipe = Pipeline([('scaler', scaler), ('mlp', mlp_down)])\n",
    "mlp_down_grid = GridSearchCV(mlp_down_pipe, mlp_param, n_jobs=-1, cv=5)\n",
    "y_down_pred = cross_val_predict(mlp_down_grid, samp_features, samp_genres, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results on the downsampled dataset:')\n",
    "print(classification_report(samp_genres, y_down_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=metrics.confusion_matrix(samp_genres,y_down_pred)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm, annot=True, fmt='g')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "<p>We decided to use a Neural Network because it is good to use when the data has a lot of inputs and there is a large quanitity of training data. Additionally, it does not suffer from the curse of dimensionality which is good for us since our data has a lot of features.\n",
    "\n",
    "Using the entire dataset with 42305 records, the MLPClassifier predicted the correct genre with 66% accuracy.\n",
    "\n",
    "Using the dataset with anomalies removed (according to the anomaly detection process from earlier), the model's accuracy increased slightly to 68%. This could be because the records that were considered edge cases (most unlike the other records in that class) have been removed so there are fewer records that are more likely to be misclassified by the model.\n",
    "\n",
    "Using the downsampled dataset of size 5490 to mitigate for class imbalance (sampled same number of records from each class), the model's accuracy decreased to 59%. This could be because since the records from various classes are so similar (as proven by analysis on the clustering), having a lot of data was a strength of the dataset that allowed the model to pick up on more minute differences between records of different classes the more it trained. Now that there are much fewer records from each classes, it is harder for the model to differentiate between classes as well as before and it is more likely to misclassify.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
